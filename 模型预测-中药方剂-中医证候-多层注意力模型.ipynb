{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b5bc70-3f6d-4696-9663-cb70a6081fe3",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c179774f-6d22-4285-9433-5e7f6b16b2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 加载图数据和标签\n",
    "work_dir = r'D:\\博士文件\\博士毕业课题材料\\维吾尔医药配伍机制量化分析\\data'\n",
    "merged_file = os.path.join(work_dir, 'all_graphs_to_be_predicted.pt')\n",
    "merged_graphs = torch.load(merged_file)\n",
    "len(merged_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d009b4-4457-4659-b6cc-9eb786f6894f",
   "metadata": {},
   "source": [
    "# 多层注意力模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224152d0-c843-4c03-b9a7-54b965c599e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GATModel(\n",
      "  (layer1): GATConv(91, 64, heads=2)\n",
      "  (layer2): GATConv(128, 64, heads=2)\n",
      "  (layer3): GATConv(128, 64, heads=2)\n",
      "  (layer4): GATConv(128, 64, heads=1)\n",
      "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# 设置随机种子\n",
    "def set_seed(seed):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # 如果使用GPU\n",
    "\n",
    "set_seed(42)  # 设置你的随机种子\n",
    "\n",
    "# GAT 模型定义\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_heads, dropout_rate=0.3, dosage_weight=1.0):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.dosage_weight = dosage_weight  # 用于放大第91号特征的权重\n",
    "        \n",
    "        # 增加四层 GAT 注意力机制\n",
    "        self.layer1 = GATConv(in_dim, hidden_dim, heads=num_heads, dropout=dropout_rate)\n",
    "        self.layer2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout_rate)\n",
    "        self.layer3 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout_rate)\n",
    "        self.layer4 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1, dropout=dropout_rate)  # 第四层 GAT\n",
    "        \n",
    "        # 全连接层，用于最终输出\n",
    "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "        # 权重初始化\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for layer in [self.layer1, self.layer2, self.layer3, self.layer4]:  # 加入第四层的权重初始化\n",
    "            nn.init.xavier_uniform_(layer.lin.weight)  # 线性层权重初始化\n",
    "            if layer.lin.bias is not None:\n",
    "                nn.init.zeros_(layer.lin.bias)  # 偏置初始化为0\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # 取出第91号特征并乘以 self.dosage_weight，并限制其放大范围\n",
    "        x[:, 90] = torch.clamp(x[:, 90] * self.dosage_weight, min=0, max=10)\n",
    "\n",
    "        # 执行 GAT 层计算并获取注意力权重\n",
    "        h, attn_weights_1 = self.layer1(x, edge_index, return_attention_weights=True)\n",
    "        h = torch.relu(h)\n",
    "\n",
    "        h, attn_weights_2 = self.layer2(h, edge_index, return_attention_weights=True)\n",
    "        h = torch.relu(h)\n",
    "\n",
    "        h, attn_weights_3 = self.layer3(h, edge_index, return_attention_weights=True)\n",
    "        h = torch.relu(h)\n",
    "\n",
    "        h, attn_weights_4 = self.layer4(h, edge_index, return_attention_weights=True)  # 计算第四层的输出和注意力权重\n",
    "        \n",
    "        # 使用全局均值池化汇聚节点信息\n",
    "        hg = global_mean_pool(h, batch)\n",
    "        \n",
    "        # 输出层\n",
    "        out = self.fc(hg)\n",
    "        \n",
    "        # 返回最终输出以及所有的注意力权重\n",
    "        return out, hg, (attn_weights_1, attn_weights_2, attn_weights_3, attn_weights_4)\n",
    "\n",
    "# 模型参数设置\n",
    "in_dim = 91       # 节点特征维度\n",
    "hidden_dim = 64   # 隐藏层维度\n",
    "out_dim = 4       # 输出维度，对应5个标签\n",
    "num_heads = 2     # GAT多头注意力数量\n",
    "dropout_rate = 0.4\n",
    "dosage_weight = 1  # 放大剂量参数的权重\n",
    "\n",
    "# 构建模型\n",
    "model = GATModel(in_dim, hidden_dim, out_dim, num_heads, dropout_rate, dosage_weight=dosage_weight)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1fe649-e3b0-4b9c-a0e5-0386f273d014",
   "metadata": {},
   "source": [
    "# 模型 加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fadf07-ffde-4331-a2f4-4c17c6992252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# 删除模型\n",
    "del model  # 删除模型对象\n",
    "gc.collect()  # 强制进行垃圾回收，释放内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e5fa03a-f96e-40c2-93ac-12c087b111f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from D:\\博士文件\\博士毕业课题材料\\维吾尔医药配伍机制量化分析\\data\\gat_model.pth\n"
     ]
    }
   ],
   "source": [
    "#加载模型\n",
    "model = GATModel(in_dim, hidden_dim, out_dim, num_heads, dropout_rate, dosage_weight=dosage_weight)\n",
    "\n",
    "# 直接加载整个模型\n",
    "model_save_path = r'D:\\博士文件\\博士毕业课题材料\\维吾尔医药配伍机制量化分析\\data\\gat_model.pth'\n",
    "model = torch.load(model_save_path)  # 直接加载整个模型\n",
    "model.eval()  # 切换到评估模式\n",
    "print(f\"Model loaded from {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34529ae4-a2a4-442c-aadf-291d541f1e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a55c365-6015-4ab9-9d44-ca87ef6476e3",
   "metadata": {},
   "source": [
    "# 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49714928-b815-471c-888c-22d224a12c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction outputs exported to D:\\博士文件\\博士毕业课题材料\\维吾尔医药配伍机制量化分析\\data\\prediction_outputs.tsv as TSV\n",
      "Attention weights exported to D:\\博士文件\\博士毕业课题材料\\维吾尔医药配伍机制量化分析\\data\\attention_weights.tsv as TSV\n"
     ]
    }
   ],
   "source": [
    "#输出为TSV文件\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 定义一个辅助函数，用于将数值保留四位有效数字\n",
    "def format_value(val):\n",
    "    return round(val, 4)\n",
    "\n",
    "# 定义预测函数\n",
    "def predict_samples(start_index, end_index):\n",
    "    output_results = []\n",
    "    attn_results = []\n",
    "\n",
    "    for i in range(start_index, end_index + 1):\n",
    "        sample = merged_graphs[i]\n",
    "        cpm_id = sample.cpm_id  # 每个样本的唯一标识名\n",
    "        out, hg, attn_weights = model(sample)\n",
    "        \n",
    "        # 对 logits 应用 sigmoid 转换为概率\n",
    "        out_probs = torch.sigmoid(out).detach().cpu().numpy()\n",
    "\n",
    "        # 第一个表的数据：cpm_id, out_probs（转为概率） 和 hg，将张量转换为四位有效数字的列表，并从 1 开始计数\n",
    "        output_result = {\n",
    "            \"cpm_id\": cpm_id,\n",
    "            **{f\"Class_{j+1}\": format_value(val) for j, val in enumerate(out_probs.flatten())},  # 使用概率\n",
    "            **{f\"hg_{j+1}\": format_value(val) for j, val in enumerate(hg.detach().cpu().numpy().flatten())}\n",
    "        }        \n",
    "\n",
    "        output_results.append(output_result)\n",
    "\n",
    "        # 第二个表的数据：cpm_id，节点名称和注意力权重\n",
    "        node_names = sample.node_names  # 确保 graph 有 `node_names` 属性\n",
    "\n",
    "        # 初始化一个字典来存储边和对应的注意力权重\n",
    "        edge_dict = {}\n",
    "\n",
    "        # 遍历每一层的注意力权重\n",
    "        for layer_idx, (edge_index, attn_weight) in enumerate(attn_weights, start=1):\n",
    "            edge_index_np = edge_index.detach().cpu().numpy()  # [2, E]\n",
    "            attn_weight_np = attn_weight.detach().cpu().numpy()  # [E, heads]\n",
    "\n",
    "            # 转置 edge_index_np，得到 [E, 2]\n",
    "            edges = edge_index_np.T\n",
    "\n",
    "            # 对于每个边，获取节点名称和对应的权重\n",
    "            for edge, attn in zip(edges, attn_weight_np):\n",
    "                node_idx_1, node_idx_2 = edge\n",
    "                node_name_1 = node_names[int(node_idx_1)]\n",
    "                node_name_2 = node_names[int(node_idx_2)]\n",
    "\n",
    "                edge_key = (node_name_1, node_name_2)\n",
    "\n",
    "                # 初始化字典\n",
    "                if edge_key not in edge_dict:\n",
    "                    edge_dict[edge_key] = {\n",
    "                        \"cpm_id\": cpm_id,\n",
    "                        \"Source\": node_name_1,\n",
    "                        \"Target\": node_name_2\n",
    "                    }\n",
    "\n",
    "                # 将注意力权重展开，每个注意力头占一个单元格，注意从 1 开始计数\n",
    "                for head_idx, attn_value in enumerate(attn, start=1):\n",
    "                    attn_col_name = f\"attn_weights_{layer_idx}_head_{head_idx}\"\n",
    "                    edge_dict[edge_key][attn_col_name] = format_value(attn_value)\n",
    "\n",
    "        # 将 edge_dict 的值添加到 attn_results 列表中\n",
    "        attn_results.extend(edge_dict.values())\n",
    "\n",
    "    # 导出第一个表为 TSV 文件\n",
    "    output_df = pd.DataFrame(output_results)\n",
    "    output_path = os.path.join(work_dir, 'prediction_outputs.tsv')\n",
    "    output_df.to_csv(output_path, sep='\\t', index=False)\n",
    "    print(f\"Prediction outputs exported to {output_path} as TSV\")\n",
    "\n",
    "    # 导出第二个表为 TSV 文件\n",
    "    attn_df = pd.DataFrame(attn_results)\n",
    "    attn_path = os.path.join(work_dir, 'attention_weights.tsv')\n",
    "    attn_df.to_csv(attn_path, sep='\\t', index=False)\n",
    "    print(f\"Attention weights exported to {attn_path} as TSV\")\n",
    "\n",
    "# 自定义预测范围\n",
    "start_index = 0  # 你可以更改这个值\n",
    "end_index = 479  # 你可以更改这个值\n",
    "predict_samples(start_index, end_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3561ae28-35ef-40fa-bc65-13a238d4e33b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
