{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b5bc70-3f6d-4696-9663-cb70a6081fe3",
   "metadata": {},
   "source": [
    "# Graph Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c179774f-6d22-4285-9433-5e7f6b16b2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define working and data directories\n",
    "# This assumes the graph data is located in a relative '../Data' directory\n",
    "work_dir = os.getcwd()  # Retrieve the current working directory\n",
    "input_data_dir = os.path.join(work_dir, '../Data')  # Define input directory for graph data\n",
    "output_dir = os.path.join(work_dir, '../Data')      # Define output directory (same as input in this context)\n",
    "\n",
    "# Load the preprocessed graph dataset (PyTorch Geometric format)\n",
    "# The file is expected to contain a list of graphs prepared for model inference or training\n",
    "merged_file = os.path.join(input_data_dir, 'all_graphs_to_be_predicted.pt')\n",
    "merged_graphs = torch.load(merged_file)\n",
    "\n",
    "# Output the number of graphs successfully loaded\n",
    "len(merged_graphs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d009b4-4457-4659-b6cc-9eb786f6894f",
   "metadata": {},
   "source": [
    "# GAT Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "224152d0-c843-4c03-b9a7-54b965c599e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GATModel(\n",
      "  (layer1): GATConv(91, 64, heads=2)\n",
      "  (layer2): GATConv(128, 64, heads=2)\n",
      "  (layer3): GATConv(128, 64, heads=2)\n",
      "  (layer4): GATConv(128, 64, heads=1)\n",
      "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Function to fix random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # Ensures GPU-level reproducibility if applicable\n",
    "\n",
    "set_seed(42)  # Set your preferred random seed value\n",
    "\n",
    "# Definition of the Graph Attention Network (GAT) model\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_heads, dropout_rate=0.3, dosage_weight=1.0):\n",
    "        \"\"\"\n",
    "        GAT-based neural network for graph-level prediction tasks.\n",
    "\n",
    "        Parameters:\n",
    "            in_dim (int): Input feature dimension for each node.\n",
    "            hidden_dim (int): Hidden dimension for GAT layers.\n",
    "            out_dim (int): Output dimension (e.g., number of classes or regression targets).\n",
    "            num_heads (int): Number of attention heads in each GAT layer.\n",
    "            dropout_rate (float): Dropout rate applied to GAT layers.\n",
    "            dosage_weight (float): Scaling factor for the 91st feature (dosage ratio).\n",
    "        \"\"\"\n",
    "        super(GATModel, self).__init__()\n",
    "        self.dosage_weight = dosage_weight  # Weighting factor for dosage feature (feature 91)\n",
    "\n",
    "        # Stack of 4 GATConv layers for deep graph feature extraction\n",
    "        self.layer1 = GATConv(in_dim, hidden_dim, heads=num_heads, dropout=dropout_rate)\n",
    "        self.layer2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout_rate)\n",
    "        self.layer3 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout_rate)\n",
    "        self.layer4 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1, dropout=dropout_rate)  # Last GAT layer\n",
    "\n",
    "        # Fully connected layer to map pooled graph-level representation to final output\n",
    "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "        # Initialize weights using Xavier uniform initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Apply Xavier initialization to linear components of GAT layers.\n",
    "        \"\"\"\n",
    "        for layer in [self.layer1, self.layer2, self.layer3, self.layer4]:\n",
    "            nn.init.xavier_uniform_(layer.lin.weight)\n",
    "            if layer.lin.bias is not None:\n",
    "                nn.init.zeros_(layer.lin.bias)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass of the GAT model.\n",
    "\n",
    "        Parameters:\n",
    "            data (torch_geometric.data.Data): A batch of graph data containing x, edge_index, and batch attributes.\n",
    "\n",
    "        Returns:\n",
    "            out (Tensor): Final prediction output.\n",
    "            hg (Tensor): Graph-level pooled embeddings.\n",
    "            attn_weights (Tuple): Attention weights from each GAT layer for interpretability.\n",
    "        \"\"\"\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # Amplify dosage-related feature (91st feature) using the specified weight, capped between 0 and 10\n",
    "        x[:, 90] = torch.clamp(x[:, 90] * self.dosage_weight, min=0, max=10)\n",
    "\n",
    "        # Apply first GAT layer\n",
    "        h, attn_weights_1 = self.layer1(x, edge_index, return_attention_weights=True)\n",
    "        h = torch.relu(h)\n",
    "\n",
    "        # Apply second GAT layer\n",
    "        h, attn_weights_2 = self.layer2(h, edge_index, return_attention_weights=True)\n",
    "        h = torch.relu(h)\n",
    "\n",
    "        # Apply third GAT layer\n",
    "        h, attn_weights_3 = self.layer3(h, edge_index, return_attention_weights=True)\n",
    "        h = torch.relu(h)\n",
    "\n",
    "        # Apply fourth GAT layer (single-head output)\n",
    "        h, attn_weights_4 = self.layer4(h, edge_index, return_attention_weights=True)\n",
    "\n",
    "        # Aggregate node features to obtain graph-level embedding\n",
    "        hg = global_mean_pool(h, batch)\n",
    "\n",
    "        # Fully connected prediction layer\n",
    "        out = self.fc(hg)\n",
    "\n",
    "        # Return output, graph embedding, and attention weights for analysis\n",
    "        return out, hg, (attn_weights_1, attn_weights_2, attn_weights_3, attn_weights_4)\n",
    "\n",
    "# ---------------- Model Instantiation ----------------\n",
    "\n",
    "# Define model hyperparameters\n",
    "in_dim = 91        # Input feature dimension (91 features per node)\n",
    "hidden_dim = 64    # Hidden size for GAT layers\n",
    "out_dim = 4        # Output dimension (e.g., 4 therapeutic effect categories)\n",
    "num_heads = 2      # Number of attention heads\n",
    "dropout_rate = 0.4 # Dropout rate during training\n",
    "dosage_weight = 1  # Dosage amplification factor (for feature 91)\n",
    "\n",
    "# Instantiate the GAT model\n",
    "model = GATModel(in_dim, hidden_dim, out_dim, num_heads, dropout_rate, dosage_weight=dosage_weight)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1fe649-e3b0-4b9c-a0e5-0386f273d014",
   "metadata": {},
   "source": [
    "# GAT Model Loading and Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2fadf07-ffde-4331-a2f4-4c17c6992252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from D:\\博士文件\\博士毕业课题材料\\维吾尔医药配伍机制量化分析\\GraphAI-for-Uyghur-Medicine\\Python\\../Data\\gat_model.pth\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Model Loading and Memory Cleanup ----------------------\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Explicitly remove existing model instance to release memory\n",
    "del model  # Delete current model instance from memory\n",
    "gc.collect()  # Manually trigger Python garbage collection to free GPU/CPU memory\n",
    "\n",
    "# Reconstruct the GAT model architecture\n",
    "# This step is required prior to loading state if the model was saved via state_dict\n",
    "model = GATModel(in_dim, hidden_dim, out_dim, num_heads, dropout_rate, dosage_weight=dosage_weight)\n",
    "\n",
    "# Define the working and data directories\n",
    "work_dir = os.getcwd()  # Get the current working directory (usually project root)\n",
    "input_data_dir = os.path.join(work_dir, '../Data')  # Directory containing saved model files\n",
    "output_dir = os.path.join(work_dir, '../Data')      # Output directory (shared)\n",
    "\n",
    "# Specify the path to the saved model file (.pth format)\n",
    "model_save_path = os.path.join(input_data_dir, 'gat_model.pth')\n",
    "\n",
    "# Load the pre-trained model from file\n",
    "# Note: This assumes the full model (not just state_dict) was saved via torch.save(model)\n",
    "model = torch.load(model_save_path)\n",
    "\n",
    "# Set the model to evaluation mode (disable dropout, etc.)\n",
    "model.eval()\n",
    "\n",
    "# Confirm successful loading\n",
    "print(f\"Model loaded from {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34529ae4-a2a4-442c-aadf-291d541f1e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a55c365-6015-4ab9-9d44-ca87ef6476e3",
   "metadata": {},
   "source": [
    "# Model Inference and TSV Output for Predictions and Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49714928-b815-471c-888c-22d224a12c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction outputs exported to D:\\博士文件\\博士毕业课题材料\\维吾尔医药配伍机制量化分析\\GraphAI-for-Uyghur-Medicine\\Python\\../Data\\prediction_outputs.tsv as TSV\n",
      "Attention weights exported to D:\\博士文件\\博士毕业课题材料\\维吾尔医药配伍机制量化分析\\GraphAI-for-Uyghur-Medicine\\Python\\../Data\\attention_weights.tsv as TSV\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- GAT Model Inference and Result Export ----------------------\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Utility function to format floating point numbers to 4 decimal places\n",
    "def format_value(val):\n",
    "    return round(val, 4)\n",
    "\n",
    "# Define the prediction routine for a given range of graph samples\n",
    "def predict_samples(start_index, end_index):\n",
    "    output_results = []  # Stores prediction scores and graph-level embeddings\n",
    "    attn_results = []    # Stores attention weights for all graph edges\n",
    "\n",
    "    # Iterate over the specified range of graphs\n",
    "    for i in range(start_index, end_index + 1):\n",
    "        sample = merged_graphs[i]\n",
    "        cpm_id = sample.cpm_id  # Unique prescription identifier\n",
    "\n",
    "        # Run the model in evaluation mode\n",
    "        out, hg, attn_weights = model(sample)\n",
    "\n",
    "        # Apply sigmoid to convert raw logits to probability values\n",
    "        out_probs = torch.sigmoid(out).detach().cpu().numpy()\n",
    "\n",
    "        # First output table: prediction scores and graph-level embeddings\n",
    "        output_result = {\n",
    "            \"cpm_id\": cpm_id,\n",
    "            **{f\"Class_{j+1}\": format_value(val) for j, val in enumerate(out_probs.flatten())},\n",
    "            **{f\"hg_{j+1}\": format_value(val) for j, val in enumerate(hg.detach().cpu().numpy().flatten())}\n",
    "        }\n",
    "        output_results.append(output_result)\n",
    "\n",
    "        # Second output table: edge-wise attention weights\n",
    "        node_names = sample.node_names  # Ensure node names are available\n",
    "\n",
    "        edge_dict = {}  # Dictionary to collect attention data for each edge\n",
    "\n",
    "        # Loop over each attention layer's output\n",
    "        for layer_idx, (edge_index, attn_weight) in enumerate(attn_weights, start=1):\n",
    "            edge_index_np = edge_index.detach().cpu().numpy()  # Shape: [2, E]\n",
    "            attn_weight_np = attn_weight.detach().cpu().numpy()  # Shape: [E, num_heads]\n",
    "\n",
    "            edges = edge_index_np.T  # Convert to shape [E, 2] for easier iteration\n",
    "\n",
    "            # Iterate over all edges and collect attention scores\n",
    "            for edge, attn in zip(edges, attn_weight_np):\n",
    "                node_idx_1, node_idx_2 = edge\n",
    "                node_name_1 = node_names[int(node_idx_1)]\n",
    "                node_name_2 = node_names[int(node_idx_2)]\n",
    "\n",
    "                edge_key = (node_name_1, node_name_2)\n",
    "\n",
    "                # Initialize or update the dictionary for this edge\n",
    "                if edge_key not in edge_dict:\n",
    "                    edge_dict[edge_key] = {\n",
    "                        \"cpm_id\": cpm_id,\n",
    "                        \"Source\": node_name_1,\n",
    "                        \"Target\": node_name_2\n",
    "                    }\n",
    "\n",
    "                # Record attention weights per head\n",
    "                for head_idx, attn_value in enumerate(attn, start=1):\n",
    "                    attn_col_name = f\"attn_weights_{layer_idx}_head_{head_idx}\"\n",
    "                    edge_dict[edge_key][attn_col_name] = format_value(attn_value)\n",
    "\n",
    "        # Append attention results to list\n",
    "        attn_results.extend(edge_dict.values())\n",
    "\n",
    "    # Convert predictions to DataFrame and export as TSV\n",
    "    output_df = pd.DataFrame(output_results)\n",
    "    output_path = os.path.join(output_dir, 'prediction_outputs.tsv')\n",
    "    output_df.to_csv(output_path, sep='\\t', index=False)\n",
    "    print(f\"Prediction outputs exported to {output_path} as TSV\")\n",
    "\n",
    "    # Convert attention data to DataFrame and export as TSV\n",
    "    attn_df = pd.DataFrame(attn_results)\n",
    "    attn_path = os.path.join(output_dir, 'attention_weights.tsv')\n",
    "    attn_df.to_csv(attn_path, sep='\\t', index=False)\n",
    "    print(f\"Attention weights exported to {attn_path} as TSV\")\n",
    "\n",
    "# ---------------------- Inference Execution ----------------------\n",
    "\n",
    "# Specify the prediction range (inclusive)\n",
    "start_index = 0  # Start from the first graph\n",
    "end_index = 5    # Predict for first 6 graphs (0 through 5)\n",
    "\n",
    "# Execute inference and export results\n",
    "predict_samples(start_index, end_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3561ae28-35ef-40fa-bc65-13a238d4e33b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
