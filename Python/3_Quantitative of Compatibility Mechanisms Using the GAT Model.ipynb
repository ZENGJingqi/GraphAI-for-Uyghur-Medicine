{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece1a7f8-a40f-4223-9b13-ea795a696529",
   "metadata": {},
   "source": [
    "# Compute the Average Attention Weights for Each Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b044ac13-f3e6-417c-acf5-b1804533a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------- Set Up Working Environment ----------------------\n",
    "\n",
    "# Define working directory and input/output paths\n",
    "work_dir = os.getcwd()  # Use the current working directory\n",
    "input_data_dir = os.path.join(work_dir, '../Data')  # Input data directory\n",
    "output_dir = os.path.join(work_dir, '../Data')      # Output data directory\n",
    "os.chdir(input_data_dir)  # Change current working directory for file access\n",
    "\n",
    "# ---------------------- Load Edge-Level Attention Weights ----------------------\n",
    "\n",
    "# Read the TSV file containing raw attention weights from GAT layers\n",
    "# Each row represents an edge in a graph with attention values per head per layer\n",
    "SD10data = pd.read_csv(\"attention_weights.tsv\", sep=\"\\t\")\n",
    "\n",
    "# ---------------------- Compute Mean Attention per Layer ----------------------\n",
    "\n",
    "# For each layer, compute the mean attention weight across all heads\n",
    "# Round to 4 decimal places for concise representation\n",
    "\n",
    "SD10data['attn_layer_1_avg'] = SD10data.filter(regex='^attn_weights_1_head').mean(axis=1).round(4)\n",
    "SD10data['attn_layer_2_avg'] = SD10data.filter(regex='^attn_weights_2_head').mean(axis=1).round(4)\n",
    "SD10data['attn_layer_3_avg'] = SD10data.filter(regex='^attn_weights_3_head').mean(axis=1).round(4)\n",
    "SD10data['attn_layer_4_avg'] = SD10data.filter(regex='^attn_weights_4_head').mean(axis=1).round(4)\n",
    "\n",
    "# ---------------------- Prepare Output Table ----------------------\n",
    "\n",
    "# Select relevant columns for export:\n",
    "# cpm_id - prescription identifier\n",
    "# Source, Target - edge endpoints\n",
    "# Layer-wise average attention weights\n",
    "\n",
    "attention_data = SD10data[[\n",
    "    'cpm_id', 'Source', 'Target',\n",
    "    'attn_layer_1_avg',\n",
    "    'attn_layer_2_avg',\n",
    "    'attn_layer_3_avg',\n",
    "    'attn_layer_4_avg'\n",
    "]]\n",
    "\n",
    "# ---------------------- Export to TSV ----------------------\n",
    "\n",
    "# Define output file path\n",
    "output_file = os.path.join(output_dir, \"attention_averages.tsv\")\n",
    "\n",
    "# Export to TSV format for downstream analysis or visualization\n",
    "attention_data.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Average attention weights saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c717893-2998-4ddf-a8d8-450197d28e8f",
   "metadata": {},
   "source": [
    "# Calculate Mutual Attention Between Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1f9bf-2ecb-4743-adb1-ef3d7fefb5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------- Setup: Define Directory Paths ----------------------\n",
    "\n",
    "# Define working directory and relative input/output paths\n",
    "work_dir = os.getcwd()  # Get the current working directory\n",
    "input_data_dir = os.path.join(work_dir, '../Data')  # Input directory for TSV files\n",
    "output_dir = os.path.join(work_dir, '../Data')      # Output directory for results\n",
    "\n",
    "# Set the path for the input attention file\n",
    "file_path = os.path.join(input_data_dir, \"attention_averages.tsv\")\n",
    "\n",
    "# ---------------------- Load Attention Data ----------------------\n",
    "\n",
    "# Load attention weights averaged over heads for each GAT layer\n",
    "attention_data = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Get unique prescription identifiers (cpm_id)\n",
    "cpm_ids = attention_data['cpm_id'].unique()\n",
    "\n",
    "# Prepare output file path\n",
    "output_path = os.path.join(output_dir, \"calculated_attention_weights.tsv\")\n",
    "\n",
    "# If output file exists, delete to avoid duplication\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "\n",
    "# Initialize the output file with a header\n",
    "with open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "    f_out.write('cpm_id\\tSource\\tTarget\\tattention\\n')\n",
    "\n",
    "# ---------------------- Attention Aggregation Logic ----------------------\n",
    "\n",
    "# Define batch size for writing results periodically\n",
    "batch_size = 100\n",
    "batch_results = []\n",
    "\n",
    "# Iterate over each prescription graph (cpm_id) using tqdm for progress tracking\n",
    "for idx, cpm_id in enumerate(tqdm(cpm_ids, desc=\"Processing cpm_id\")):\n",
    "    # Filter edges for the current graph\n",
    "    cpm_data = attention_data[attention_data['cpm_id'] == cpm_id]\n",
    "\n",
    "    # Get all unique node names and sort for consistency\n",
    "    nodes = sorted(pd.unique(cpm_data[['Source', 'Target']].values.ravel()))\n",
    "\n",
    "    # Initialize dictionary to store aggregated attention scores\n",
    "    attention_dict = defaultdict(float)\n",
    "\n",
    "    # Multi-layer attention tracing from layer 1 to 4 (deep paths)\n",
    "    for target in nodes:\n",
    "        fourth_layer = cpm_data[cpm_data['Target'] == target]\n",
    "\n",
    "        for _, fourth_row in fourth_layer.iterrows():\n",
    "            source_4 = fourth_row['Source']\n",
    "            weight_4 = fourth_row['attn_layer_4_avg']\n",
    "\n",
    "            third_layer = cpm_data[cpm_data['Target'] == source_4]\n",
    "\n",
    "            for _, third_row in third_layer.iterrows():\n",
    "                source_3 = third_row['Source']\n",
    "                weight_3 = third_row['attn_layer_3_avg']\n",
    "\n",
    "                second_layer = cpm_data[cpm_data['Target'] == source_3]\n",
    "\n",
    "                for _, second_row in second_layer.iterrows():\n",
    "                    source_2 = second_row['Source']\n",
    "                    weight_2 = second_row['attn_layer_2_avg']\n",
    "\n",
    "                    first_layer = cpm_data[cpm_data['Target'] == source_2]\n",
    "\n",
    "                    for _, first_row in first_layer.iterrows():\n",
    "                        source_1 = first_row['Source']\n",
    "                        weight_1 = first_row['attn_layer_1_avg']\n",
    "\n",
    "                        # Compute cumulative attention: product across all 4 layers\n",
    "                        total_weight = weight_1 * weight_2 * weight_3 * weight_4\n",
    "\n",
    "                        # Accumulate attention to the final dictionary\n",
    "                        attention_dict[(source_1, target)] += total_weight\n",
    "\n",
    "    # Convert accumulated results into a DataFrame\n",
    "    results = [[cpm_id, src, tgt, weight] for (src, tgt), weight in attention_dict.items()]\n",
    "    attention_df = pd.DataFrame(results, columns=[\"cpm_id\", \"Source\", \"Target\", \"attention\"])\n",
    "\n",
    "    # Optional: normalize attention weights per target node\n",
    "    # attention_df['attention'] = attention_df.groupby('Target')['attention'].transform(lambda x: x / x.sum())\n",
    "\n",
    "    # Round attention values for compact storage\n",
    "    attention_df['attention'] = attention_df['attention'].apply(lambda x: round(x, 6))\n",
    "\n",
    "    # Store results for periodic flushing to file\n",
    "    batch_results.append(attention_df)\n",
    "\n",
    "    # Write results every N graphs or at the end\n",
    "    if (idx + 1) % batch_size == 0 or (idx + 1) == len(cpm_ids):\n",
    "        combined_df = pd.concat(batch_results, ignore_index=True)\n",
    "        combined_df.to_csv(output_path, sep='\\t', index=False, header=False, mode='a', encoding='utf-8')\n",
    "        batch_results = []  # Reset batch buffer\n",
    "\n",
    "# ---------------------- Completion Notice ----------------------\n",
    "\n",
    "print(f\"All cpm_id attention propagation scores have been saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f99b0e0-8a26-4bbc-901c-4611afcd0d06",
   "metadata": {},
   "source": [
    "# Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188271d1-11c0-4868-aa34-0860f1011816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ---------------------- Set Up Directory Paths ----------------------\n",
    "\n",
    "# Define working directory and relative input/output paths\n",
    "work_dir = os.getcwd()  # Get the current working directory\n",
    "input_data_dir = os.path.join(work_dir, '../Data')  # Input directory for TSV files\n",
    "output_dir = os.path.join(work_dir, '../Figure')      # Output directory for results\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ---------------------- Load Required Data ----------------------\n",
    "\n",
    "# Load metadata for herbal pieces (optional use for annotations)\n",
    "D6_data = pd.read_csv(os.path.join(input_data_dir, \"Uighur_herbal_pieces.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# Load precomputed multi-layer attention propagation scores\n",
    "attention_weights = pd.read_csv(os.path.join(input_data_dir, \"calculated_attention_weights.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# ---------------------- Filter Graph and Extract Matrix ----------------------\n",
    "\n",
    "# Retain only edges between valid herbal piece nodes (CHP*) and remove self-loops\n",
    "attention_filtered = attention_weights[\n",
    "    attention_weights['Source'].str.startswith(\"CHP\") &\n",
    "    attention_weights['Target'].str.startswith(\"CHP\") &\n",
    "    (attention_weights['Source'] != attention_weights['Target'])\n",
    "]\n",
    "\n",
    "# Select a specific prescription graph for visualization (by CPM_ID)\n",
    "cpm_id = \"UHF0126\"\n",
    "cpm_id_data = attention_filtered[attention_filtered['cpm_id'] == cpm_id]\n",
    "\n",
    "# Pivot data into a matrix form suitable for heatmap visualization\n",
    "heatmap_data = cpm_id_data.pivot(index='Source', columns='Target', values='attention').fillna(0)\n",
    "\n",
    "# Assign graph label for title or filename use\n",
    "pinyin_term = cpm_id\n",
    "\n",
    "# ---------------------- Configure Plot Settings ----------------------\n",
    "\n",
    "# Set font for axis labels (suitable for English or multilingual text)\n",
    "plt.rcParams['font.sans-serif'] = ['Arial']\n",
    "plt.rcParams['axes.unicode_minus'] = False  # Fix for minus sign rendering\n",
    "\n",
    "# Set figure size in inches\n",
    "pdf_width, pdf_height = 8, 6\n",
    "\n",
    "# ---------------------- Generate Heatmap Plot ----------------------\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(pdf_width, pdf_height))\n",
    "cax = ax.matshow(heatmap_data, cmap=\"Blues\")  # Render attention values as color intensity\n",
    "\n",
    "# Annotate each cell with its numeric value (rounded to 3 decimal places)\n",
    "for (i, j), val in np.ndenumerate(heatmap_data.values):\n",
    "    ax.text(j, i, f\"{val:.3f}\", ha='center', va='center', color=\"black\", fontsize=12)\n",
    "\n",
    "# Set axis tick labels (node names)\n",
    "ax.set_xticks(np.arange(len(heatmap_data.columns)))\n",
    "ax.set_yticks(np.arange(len(heatmap_data.index)))\n",
    "ax.set_xticklabels(heatmap_data.columns, rotation=45, ha='right', fontsize=14)\n",
    "ax.set_yticklabels(heatmap_data.index, fontsize=14)\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "# Add axis labels and title\n",
    "plt.xlabel(\"Target\", fontsize=18, labelpad=10)\n",
    "plt.ylabel(\"Source\", fontsize=18, labelpad=10)\n",
    "plt.title(f'{pinyin_term}', fontsize=20, pad=20)\n",
    "\n",
    "# Remove color bar to simplify layout\n",
    "fig.colorbar(cax, ax=ax).remove()\n",
    "\n",
    "# Apply tight layout for minimal white space\n",
    "plt.tight_layout()\n",
    "\n",
    "# ---------------------- Save and Display Figure ----------------------\n",
    "\n",
    "# Save figure as a high-resolution PDF file\n",
    "plt.savefig(\n",
    "    os.path.join(output_dir, f\"{pinyin_term}_Attention_Heatmap.pdf\"),\n",
    "    format='pdf',\n",
    "    bbox_inches='tight',\n",
    "    pad_inches=0.1,\n",
    "    dpi=300\n",
    ")\n",
    "\n",
    "# Show plot in notebook or window\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
