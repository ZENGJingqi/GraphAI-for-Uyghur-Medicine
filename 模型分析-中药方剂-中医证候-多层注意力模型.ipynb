{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece1a7f8-a40f-4223-9b13-ea795a696529",
   "metadata": {},
   "source": [
    "# 计算节点之间的注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b044ac13-f3e6-417c-acf5-b1804533a8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average attention weights saved to D:/博士文件/博士毕业课题材料/维吾尔医药配伍机制量化分析/data/attention_averages.tsv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 设置工作目录\n",
    "work_dir = r'D:/博士文件/博士毕业课题材料/维吾尔医药配伍机制量化分析/data/'\n",
    "os.chdir(work_dir)\n",
    "\n",
    "# 读取数据\n",
    "SD10data = pd.read_csv(\"attention_weights.tsv\", sep=\"\\t\")\n",
    "\n",
    "# 计算每层的平均注意力并保留四位有效数字\n",
    "SD10data['attn_layer_1_avg'] = SD10data.filter(regex='^attn_weights_1_head').mean(axis=1).round(4)\n",
    "SD10data['attn_layer_2_avg'] = SD10data.filter(regex='^attn_weights_2_head').mean(axis=1).round(4)\n",
    "SD10data['attn_layer_3_avg'] = SD10data.filter(regex='^attn_weights_3_head').mean(axis=1).round(4)\n",
    "SD10data['attn_layer_4_avg'] = SD10data.filter(regex='^attn_weights_4_head').mean(axis=1).round(4)\n",
    "\n",
    "# 选择需要的列，包括 Source、Target 和三层的平均注意力\n",
    "attention_data = SD10data[['cpm_id','Source', 'Target', 'attn_layer_1_avg', 'attn_layer_2_avg', 'attn_layer_3_avg', 'attn_layer_4_avg']]\n",
    "\n",
    "# 将结果输出为新的 TSV 文件\n",
    "output_file = os.path.join(work_dir, \"attention_averages.tsv\")\n",
    "attention_data.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Average attention weights saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c717893-2998-4ddf-a8d8-450197d28e8f",
   "metadata": {},
   "source": [
    "# 计算节点相互注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf1f9bf-2ecb-4743-adb1-ef3d7fefb5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cpm_id: 100%|█████████████████████████████████████████████████████████████| 480/480 [05:26<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有 cpm_id 的注意力权重已保存至 D:/博士文件/博士毕业课题材料/维吾尔医药配伍机制量化分析/data/calculated_attention_weights.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置工作目录路径\n",
    "work_dir = r'D:/博士文件/博士毕业课题材料/维吾尔医药配伍机制量化分析/data/'\n",
    "file_path = os.path.join(work_dir, \"attention_averages.tsv\")\n",
    "\n",
    "# 读取平均注意力数据\n",
    "attention_data = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# 获取所有的 cpm_id\n",
    "cpm_ids = attention_data['cpm_id'].unique()\n",
    "\n",
    "# 准备输出文件路径\n",
    "output_path = os.path.join(work_dir, \"calculated_attention_weights.tsv\")\n",
    "\n",
    "# 如果输出文件已存在，删除它（防止旧数据干扰）\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "\n",
    "# 初始化输出文件，写入表头\n",
    "with open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "    f_out.write('cpm_id\\tSource\\tTarget\\tattention\\n')\n",
    "\n",
    "# 设置批量处理的数量\n",
    "batch_size = 100\n",
    "batch_results = []\n",
    "\n",
    "# 使用 tqdm 显示进度条\n",
    "for idx, cpm_id in enumerate(tqdm(cpm_ids, desc=\"Processing cpm_id\")):\n",
    "    cpm_data = attention_data[attention_data['cpm_id'] == cpm_id]\n",
    "\n",
    "    # 获取所有的节点并排序\n",
    "    nodes = sorted(pd.unique(cpm_data[['Source', 'Target']].values.ravel()))\n",
    "\n",
    "    # 使用 defaultdict 累加权重\n",
    "    attention_dict = defaultdict(float)\n",
    "\n",
    "    # 逐层计算注意力贡献\n",
    "    for target in nodes:\n",
    "        # 第四层：直接连接到目标节点的所有节点\n",
    "        fourth_layer = cpm_data[cpm_data['Target'] == target]\n",
    "\n",
    "        for _, fourth_row in fourth_layer.iterrows():\n",
    "            source_4 = fourth_row['Source']\n",
    "            weight_4 = fourth_row['attn_layer_4_avg']\n",
    "\n",
    "            # 第三层：连接到第四层节点的节点\n",
    "            third_layer = cpm_data[cpm_data['Target'] == source_4]\n",
    "\n",
    "            for _, third_row in third_layer.iterrows():\n",
    "                source_3 = third_row['Source']\n",
    "                weight_3 = third_row['attn_layer_3_avg']\n",
    "\n",
    "                # 第二层：连接到第三层节点的节点\n",
    "                second_layer = cpm_data[cpm_data['Target'] == source_3]\n",
    "\n",
    "                for _, second_row in second_layer.iterrows():\n",
    "                    source_2 = second_row['Source']\n",
    "                    weight_2 = second_row['attn_layer_2_avg']\n",
    "\n",
    "                    # 第一层：连接到第二层节点的节点\n",
    "                    first_layer = cpm_data[cpm_data['Target'] == source_2]\n",
    "\n",
    "                    for _, first_row in first_layer.iterrows():\n",
    "                        source_1 = first_row['Source']\n",
    "                        weight_1 = first_row['attn_layer_1_avg']\n",
    "\n",
    "                        # 计算总权重（四层的乘积）\n",
    "                        total_weight = weight_1 * weight_2 * weight_3 * weight_4\n",
    "\n",
    "                        # 累加到字典\n",
    "                        attention_dict[(source_1, target)] += total_weight\n",
    "\n",
    "    # 将字典转换为 DataFrame\n",
    "    results = [[cpm_id, src, tgt, weight] for (src, tgt), weight in attention_dict.items()]\n",
    "    attention_df = pd.DataFrame(results, columns=[\"cpm_id\", \"Source\", \"Target\", \"attention\"])\n",
    "\n",
    "    # 对每个目标节点的注意力进行标准化（可选）\n",
    "    #attention_df['attention'] = attention_df.groupby('Target')['attention'].transform(lambda x: x / x.sum())\n",
    "\n",
    "    # 保留四位有效数字\n",
    "    attention_df['attention'] = attention_df['attention'].apply(lambda x: round(x, 6))\n",
    "\n",
    "    # 将结果添加到批量结果列表\n",
    "    batch_results.append(attention_df)\n",
    "\n",
    "    # 每处理 batch_size 个 cpm_id，将数据写入文件并清空批量结果\n",
    "    if (idx + 1) % batch_size == 0 or (idx + 1) == len(cpm_ids):\n",
    "        # 将批量结果列表合并为一个 DataFrame\n",
    "        combined_df = pd.concat(batch_results, ignore_index=True)\n",
    "        # 追加保存到文件\n",
    "        combined_df.to_csv(output_path, sep='\\t', index=False, header=False, mode='a', encoding='utf-8')\n",
    "        # 清空批量结果列表\n",
    "        batch_results = []\n",
    "\n",
    "# 输出完成信息\n",
    "print(f\"所有 cpm_id 的注意力权重已保存至 {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188271d1-11c0-4868-aa34-0860f1011816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
